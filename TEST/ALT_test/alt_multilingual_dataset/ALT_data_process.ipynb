{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4b5cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ T·ªïng s·ªë c√¢u song song (√≠t nh·∫•t EN + 1 ng√¥n ng·ªØ kh√°c): 20,106\n",
      "\n",
      "üìò Dataset song song ƒëa ng√¥n ng·ªØ:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'id', 'ms', 'fil', 'khm', 'lo', 'th', 'my', 'vi'],\n",
      "        num_rows: 19101\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'id', 'ms', 'fil', 'khm', 'lo', 'th', 'my', 'vi'],\n",
      "        num_rows: 1005\n",
      "    })\n",
      "})\n",
      "\n",
      "üîç V√≠ d·ª•:\n",
      "{'en': 'They are alleged to have defamed top executives of the English daily.', 'id': 'Mereka diduga telah memfitnah eksekutif puncak harian berbahasa Inggris tersebut.', 'ms': 'Mereka dituduh telah memfitnah eksekutif atasan akhbar harian Inggeris tersebut.', 'fil': 'Ang mga ito ay sinasabing nanirang puri sa mga nakatataas na opisyal ng pahayagang Ingles.', 'khm': '·ûñ·ûΩ·ûÄ·ûÇ·û∂·ûè·üã·ûè·üí·ûö·ûº·ûú·ûè·üí·ûö·ûº·ûú·ûî·û∂·ûì·ûÖ·üÑ·ûë·ûî·üí·ûö·ûÄ·û∂·ûì·üã·ûê·û∂·ûî·û∂·ûì·ûö·û∑·üá·ûÇ·ûì·üã·ûõ·ûæ·û¢·üí·ûì·ûÄ·ûÄ·üÜ·ûñ·ûº·ûõ·ûì·üÉ·ûÄ·û∂·ûü·üÇ·ûè·ûö·ûî·ûü·üã·û¢·ûÑ·üã·ûÇ·üí·ûõ·üÅ·ûü·üî', 'lo': '‡∫û‡∫ß‡∫Å‡ªÄ‡∫Ç‡∫ª‡∫≤‡ªÄ‡∫à‡∫ª‡ªâ‡∫≤‡ªÑ‡∫î‡ªâ ‡∫Å‡ªà‡∫≤‡∫ß‡∫´‡∫≤‡∫ß‡ªà‡∫≤‡∫ó‡∫≥‡∫•‡∫≤‡∫ç‡∫ä‡∫∑‡ªà‡∫™‡∫Ω‡∫á ‡∫ú‡∫π‡ªâ‡∫ö‡ªç‡∫•‡∫¥‡∫´‡∫≤‡∫ô‡∫™‡∫π‡∫á‡∫™‡∫∏‡∫î‡∫Ç‡∫≠‡∫á ‡ªú‡∫±‡∫á‡∫™‡∫∑‡∫û‡∫¥‡∫°‡∫•‡∫≤‡∫ç‡∫ß‡∫±‡∫ô‡∫Ç‡∫≠‡∫á ‡∫≠‡∫±‡∫á‡∫Å‡∫¥‡∫î.', 'th': '‡∏û‡∏ß‡∏Å‡πÄ‡∏Ç‡∏≤‡∏ñ‡∏π‡∏Å‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏´‡∏≤‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏´‡∏°‡∏¥‡πà‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ó‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏û‡∏¥‡∏°‡∏û‡πå‡∏≠‡∏¥‡∏á‡∏•‡∏¥‡∏ä‡πÄ‡∏î‡∏•‡∏µ‡πà', 'my': '·Äû·Ä∞·Äê·Ä≠·ÄØ·Ä∑ ·ÄÄ ·Äî·Ä±·Ä∑·ÄÖ·Äâ·Ä∫ ·Ä°·ÄÑ·Ä∫·Äπ·ÄÇ·Äú·Ä≠·Äï·Ä∫ ·Äû·Äê·ÄÑ·Ä∫·Ä∏·ÄÖ·Ä¨ ·Åè ·Äë·Ä≠·Äï·Ä∫·Äê·Äî·Ä∫·Ä∏ ·Ä°·Äô·Äæ·ÄØ·ÄÜ·Ä±·Ä¨·ÄÑ·Ä∫ ·Ä°·Äõ·Ä¨·Äõ·Äæ·Ä≠ ·ÄÄ·Ä≠·ÄØ ·ÄÖ·ÄΩ·Äï·Ä∫·ÄÖ·ÄΩ·Ä≤·ÄÅ·Ä≤·Ä∑·ÄÄ·Äº·Äû·Ää·Ä∫ ·Åã', 'vi': 'H·ªç b·ªã c√°o bu·ªôc ƒë√£ b√¥i nh·ªç danh d·ª± c√°c nh√† l√£nh ƒë·∫°o c·ªßa t·ªù nh·∫≠t b√°o b·∫±ng ti·∫øng Anh.'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b1189dd9f1468f8789fc21605a86d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03651f6f02eb4560a25580cf7fa98fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010dce54ddf8411e832e076f699b1146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/19101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cb168dd2a947fb91fba72b920671d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ ƒê√£ l∆∞u dataset v√†o th∆∞ m·ª•c: alt_multilingual_dataset\n",
      "  ‚îú‚îÄ‚îÄ train.jsonl\n",
      "  ‚îú‚îÄ‚îÄ test.jsonl\n",
      "  ‚îî‚îÄ‚îÄ huggingface_dataset/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "DATA_DIR = \"ALT-Parallel-Corpus-20191206/ALT-Parallel-Corpus-20191206/ALT-Parallel-Corpus-20191206\"\n",
    "OUTPUT_DIR = \"alt_multilingual_dataset\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SOUTHEAST_ASIAN_LANGS = [\"en\", \"id\", \"ms\", \"fil\", \"khm\", \"lo\", \"th\", \"my\", \"vi\"]\n",
    "TEST_RATIO = 0.05\n",
    "\n",
    "# --- H√ÄM ƒê·ªåC FILE ALT ---\n",
    "def read_alt_file(path):\n",
    "    data = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    id_, text = line.strip().split(\"\\t\", 1)\n",
    "                    data[id_] = text\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return data\n",
    "\n",
    "# --- ƒê·ªåC T·∫§T C·∫¢ NG√îN NG·ªÆ ---\n",
    "lang_datasets = {}\n",
    "for lang in SOUTHEAST_ASIAN_LANGS:\n",
    "    lang_path = os.path.join(DATA_DIR, f\"data_{lang}.txt\")\n",
    "    if not os.path.exists(lang_path):\n",
    "        print(f\"‚ö†Ô∏è Missing file for language: {lang}\")\n",
    "        lang_datasets[lang] = {}\n",
    "        continue\n",
    "    lang_datasets[lang] = read_alt_file(lang_path)\n",
    "\n",
    "# --- G·ªòP THEO ID ---\n",
    "all_ids = list(lang_datasets[\"en\"].keys())\n",
    "records = []\n",
    "\n",
    "for k in all_ids:\n",
    "    row = {}\n",
    "    has_translation = False\n",
    "    for lang in SOUTHEAST_ASIAN_LANGS:\n",
    "        text = lang_datasets.get(lang, {}).get(k, \"\").strip()\n",
    "        row[lang] = text\n",
    "        if lang != \"en\" and text:\n",
    "            has_translation = True\n",
    "    if has_translation:\n",
    "        records.append(row)\n",
    "\n",
    "print(f\"‚úÖ T·ªïng s·ªë c√¢u song song (√≠t nh·∫•t EN + 1 ng√¥n ng·ªØ kh√°c): {len(records):,}\")\n",
    "\n",
    "# --- CHIA TRAIN/TEST ---\n",
    "random.shuffle(records)\n",
    "n_test = int(len(records) * TEST_RATIO)\n",
    "test_records = records[:n_test]\n",
    "train_records = records[n_test:]\n",
    "\n",
    "multi_ds = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_records),\n",
    "    \"test\": Dataset.from_list(test_records)\n",
    "})\n",
    "\n",
    "print(\"\\nüìò Dataset song song ƒëa ng√¥n ng·ªØ:\")\n",
    "print(multi_ds)\n",
    "print(\"\\nüîç V√≠ d·ª•:\")\n",
    "print(multi_ds[\"train\"][0])\n",
    "\n",
    "# --- üíæ L∆ØU L·∫†I ƒê·ªÇ D√ôNG SAU ---\n",
    "multi_ds[\"train\"].to_json(os.path.join(OUTPUT_DIR, \"train.jsonl\"), force_ascii=False)\n",
    "multi_ds[\"test\"].to_json(os.path.join(OUTPUT_DIR, \"test.jsonl\"), force_ascii=False)\n",
    "multi_ds.save_to_disk(os.path.join(OUTPUT_DIR, \"huggingface_dataset\"))\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ l∆∞u dataset v√†o th∆∞ m·ª•c: {OUTPUT_DIR}\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ train.jsonl\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ test.jsonl\")\n",
    "print(\"  ‚îî‚îÄ‚îÄ huggingface_dataset/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb725c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Using Model: gemini-2.5-flash-lite ===\n",
      "=== TRAIN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 4/1911 [00:42<5:41:27, 10.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== TRAIN ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m     \u001b[43mprocess_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== TEST ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 135\u001b[0m, in \u001b[0;36mprocess_split\u001b[1;34m(dataset, append_file, start_idx, split_name, progress)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m         output_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    137\u001b[0m         parsed \u001b[38;5;241m=\u001b[39m safe_json_parse(output_text)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m    332\u001b[0m             request,\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_interceptor.py:329\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interceptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:79\u001b[0m, in \u001b[0;36m_LoggingClientInterceptor.intercept_unary_unary\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m     64\u001b[0m     grpc_request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_payload,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[0;32m     68\u001b[0m     }\n\u001b[0;32m     69\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m         extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         },\n\u001b[0;32m     77\u001b[0m     )\n\u001b[1;32m---> 79\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     response_metadata \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtrailing_metadata()\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     new_method,\n\u001b[0;32m    308\u001b[0m     new_timeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     new_compression,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_channel.py:1189\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1182\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1188\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[1;32m-> 1189\u001b[0m     state, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "API_KEY = \"AIzaSyB5zDpFEqzEQmBGK3axkLSqUKbNiUxzUWQ\" # H√£y b·∫£o m·∫≠t key n√†y\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\" \n",
    "\n",
    "BATCH_SIZE = 10\n",
    "OUTPUT_FILE = \"phrases.jsonl\"\n",
    "PROGRESS_FILE = \"progress.json\"\n",
    "\n",
    "# --- PROMPT ---\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a Multilingual High-Value Term Extraction Expert.\n",
    "\n",
    "\n",
    "Input: A JSON object of parallel sentences by language code.\n",
    "\n",
    "\n",
    "Goal:\n",
    "Extract ONLY English high-value terms that are difficult for NMT models to translate correctly, then align the exact corresponding expression from each target language.\n",
    "\n",
    "\n",
    "1) English Term Extraction (High-Value Only)\n",
    "- Include: domain-specific noun phrases (Engineering, Legal, Medical, Political, Business, Economics, Technology), institutional/program names, technical compounds, idioms often mistranslated by NMT (e.g., \"burden of proof\", \"chain of custody\").\n",
    "- Exclude: easy/common phrases (\"public meeting\"), place names (unless part of a technical unit like \"Paris Agreement\"), conversational language, dates/times/quantities.\n",
    "\n",
    "\n",
    "2) Structure & Quality Rules\n",
    "- Prefer bigrams/trigrams; single words only if highly technical (\"neoplasm\", \"arbitrage\").\n",
    "- Phrase must not begin or end with a stopword.\n",
    "- No pronouns; must start with noun or adjective (not a verb).\n",
    "\n",
    "\n",
    "3) Multilingual Alignment\n",
    "- For each extracted English term, return the exact matching string fragment from each target sentence.\n",
    "- If target text omits or generalizes the term, return empty string \"\".\n",
    "- For scripts without spaces (TH/LA/KM/MM), extract the full semantic unit; do not split characters.\n",
    "\n",
    "\n",
    "4) Output\n",
    "Return ONLY a JSON array of objects with keys for each language present (e.g., \"en\",\"vi\",\"th\",\"ms\",...).\n",
    "\n",
    "\n",
    "Example output item:\n",
    "{\n",
    "\"en\": \"monetary policy committee\",\n",
    "\"vi\": \"·ªßy ban ch√≠nh s√°ch ti·ªÅn t·ªá\",\n",
    "\"th\": \"\",\n",
    "\"ms\": \"jawatankuasa dasar monetari\"\n",
    "}\n",
    "\n",
    "\n",
    "INPUT: \n",
    "[[INPUT_BATCH]]\n",
    "\"\"\"\n",
    "\n",
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        try:\n",
    "            with open(PROGRESS_FILE, \"r\", encoding=\"utf8\") as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return {\"train_index\": 0, \"test_index\": 0}\n",
    "    else:\n",
    "        return {\"train_index\": 0, \"test_index\": 0}\n",
    "\n",
    "def save_progress(train_i, test_i):\n",
    "    with open(PROGRESS_FILE, \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump({\n",
    "            \"train_index\": train_i,\n",
    "            \"test_index\": test_i\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def safe_json_parse(text):\n",
    "    if not text: return None\n",
    "    # Remove markdown code blocks if present\n",
    "    text = re.sub(r\"```json|```\", \"\", text).strip()\n",
    "    \n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        # Fallback: Try to find the list brackets\n",
    "        try:\n",
    "            start = text.find('[')\n",
    "            end = text.rfind(']') + 1\n",
    "            if start != -1 and end != 0:\n",
    "                return json.loads(text[start:end])\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "def process_split(dataset, append_file, start_idx, split_name, progress):\n",
    "    # C·∫•u h√¨nh model ƒë·ªÉ √©p tr·∫£ v·ªÅ JSON\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=MODEL_NAME,\n",
    "        generation_config={\"response_mime_type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(dataset), BATCH_SIZE), desc=split_name):\n",
    "        batch = dataset[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Convert batch columns -> list of row dicts\n",
    "        # HuggingFace dataset slice tr·∫£ v·ªÅ dict c·ªßa list, c·∫ßn zip l·∫°i th√†nh list c·ªßa dict\n",
    "        keys = batch.keys()\n",
    "        # L·∫•y danh s√°ch c√°c gi√° tr·ªã t·ª´ c√°c c·ªôt\n",
    "        list_of_values = zip(*[batch[k] for k in keys])\n",
    "        \n",
    "        batch_rows = []\n",
    "        for values in list_of_values:\n",
    "            row_dict = dict(zip(keys, values))\n",
    "            # Ch·ªâ gi·ªØ l·∫°i c√°c key ng√¥n ng·ªØ c·∫ßn thi·∫øt ƒë·ªÉ gi·∫£m token input\n",
    "            filtered_row = {\n",
    "                k: row_dict[k] for k in ['en','vi','id','ms','fil','khm','lo','th','my'] \n",
    "                if k in row_dict\n",
    "            }\n",
    "            batch_rows.append(filtered_row)\n",
    "\n",
    "        inp = json.dumps(batch_rows, ensure_ascii=False, indent=2)\n",
    "        prompt = PROMPT_TEMPLATE.replace(\"[[INPUT_BATCH]]\", inp)\n",
    "\n",
    "        retry_count = 0\n",
    "        parsed = None\n",
    "        \n",
    "        while retry_count < 3:\n",
    "            try:\n",
    "                response = model.generate_content(prompt)\n",
    "                output_text = response.text\n",
    "                parsed = safe_json_parse(output_text)\n",
    "                \n",
    "                if parsed is not None and isinstance(parsed, list):\n",
    "                    break # Success\n",
    "                else:\n",
    "                    # N·∫øu parse th·∫•t b·∫°i ho·∫∑c kh√¥ng ph·∫£i list, coi l√† l·ªói\n",
    "                    raise ValueError(\"Invalid JSON format\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError at index {i}: {e}. Retrying...\")\n",
    "                time.sleep(2 * (retry_count + 1))\n",
    "                retry_count += 1\n",
    "\n",
    "        # Ghi file\n",
    "        with open(append_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            if parsed is not None:\n",
    "                # Ghi t·ª´ng item trong list ra file jsonl (m·ªói d√≤ng 1 object ho·∫∑c 1 list)\n",
    "                # V√¨ output file l√† jsonl, ta n√™n ghi t·ª´ng c·ª•m t·ª´ extracted\n",
    "                for item in parsed:\n",
    "                     f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            else:\n",
    "                # Log l·ªói n·∫øu 3 l·∫ßn retry ƒë·ªÅu t·∫°ch\n",
    "                error_log = {\"error\": \"Failed to parse\", \"input_index\": i, \"raw_output\": output_text if 'output_text' in locals() else \"\"}\n",
    "                f.write(json.dumps(error_log, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Save progress\n",
    "        if split_name == \"train\":\n",
    "            progress[\"train_index\"] = i + BATCH_SIZE\n",
    "        else:\n",
    "            progress[\"test_index\"] = i + BATCH_SIZE\n",
    "        save_progress(progress[\"train_index\"], progress[\"test_index\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset (ƒë·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng)\n",
    "    try:\n",
    "        data = load_dataset(\"json\", data_files={\n",
    "            \"train\": \"alt_multilingual_dataset/train.jsonl\",\n",
    "            \"test\": \"alt_multilingual_dataset/test.jsonl\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Kh√¥ng t√¨m th·∫•y file dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    progress = load_progress()\n",
    "\n",
    "    # X√≥a file c≈© n·∫øu ch·∫°y t·ª´ ƒë·∫ßu\n",
    "    if progress[\"train_index\"] == 0 and progress[\"test_index\"] == 0:\n",
    "        if os.path.exists(OUTPUT_FILE):\n",
    "            os.remove(OUTPUT_FILE)\n",
    "\n",
    "    print(f\"=== Using Model: {MODEL_NAME} ===\")\n",
    "\n",
    "    if \"train\" in data:\n",
    "        print(\"=== TRAIN ===\")\n",
    "        process_split(data[\"train\"], OUTPUT_FILE, progress[\"train_index\"], \"train\", progress)\n",
    "\n",
    "    if \"test\" in data:\n",
    "        print(\"=== TEST ===\")\n",
    "        process_split(data[\"test\"], OUTPUT_FILE, progress[\"test_index\"], \"test\", progress)\n",
    "\n",
    "    print(\"üéâ DONE! All results written to phrases.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a5ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Using Model: gemini-2.5-flash-lite ===\n",
      "=== TRAIN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 15/3821 [01:54<5:35:58,  5.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at index 75: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 15, model: gemini-2.5-flash-lite\n",
      "Please retry in 25.969739302s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash-lite\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]. Retrying in 1.6s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 17/3821 [02:16<8:27:20,  8.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 162\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== TRAIN ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 162\u001b[0m     \u001b[43mprocess_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== TEST ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 111\u001b[0m, in \u001b[0;36mprocess_split\u001b[1;34m(dataset, append_file, start_idx, split_name, progress)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m         parsed \u001b[38;5;241m=\u001b[39m safe_json_parse(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m    332\u001b[0m             request,\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_interceptor.py:329\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interceptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:79\u001b[0m, in \u001b[0;36m_LoggingClientInterceptor.intercept_unary_unary\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m     64\u001b[0m     grpc_request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_payload,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[0;32m     68\u001b[0m     }\n\u001b[0;32m     69\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m         extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         },\n\u001b[0;32m     77\u001b[0m     )\n\u001b[1;32m---> 79\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     response_metadata \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtrailing_metadata()\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     new_method,\n\u001b[0;32m    308\u001b[0m     new_timeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     new_compression,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_channel.py:1189\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1182\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1188\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[1;32m-> 1189\u001b[0m     state, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\grpc\\_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Please set GEMINI_API_KEY environment variable.\")\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "OUTPUT_FILE = \"phrases.jsonl\"\n",
    "PROGRESS_FILE = \"progress.json\"\n",
    "\n",
    "TARGET_LANGS = ['en','vi','id','ms','fil','khm','lo','th','my']\n",
    "\n",
    "# --- PROMPT TEMPLATE ---\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a Multilingual High-Value Term Extraction Expert.\n",
    "\n",
    "Input: A JSON object of parallel sentences by language code.\n",
    "\n",
    "Goal:\n",
    "Extract ONLY English high-value terms that are difficult for NMT models to translate correctly,\n",
    "then align the exact corresponding expression from each target language.\n",
    "\n",
    "1) English Term Extraction:\n",
    "- Include: domain-specific noun phrases (Engineering, Legal, Medical, Political, Business, Economics, Technology), institutional/program names, technical compounds, idioms often mistranslated by NMT.\n",
    "- Exclude: easy/common phrases, place names (unless part of a technical unit), conversational language, dates/times/quantities.\n",
    "\n",
    "2) Structure & Quality:\n",
    "- Prefer bigrams/trigrams; single words only if highly technical.\n",
    "- No phrase begins/ends with stopword.\n",
    "- No pronouns; must start with noun or adjective.\n",
    "\n",
    "3) Multilingual Alignment:\n",
    "- Return exact matching string from target sentence.\n",
    "- If omitted/generalized, return \"\".\n",
    "- For scripts without spaces (TH/LA/KM/MM), extract full semantic unit.\n",
    "\n",
    "4) Output:\n",
    "Return ONLY a JSON array of objects with keys for each language present.\n",
    "\n",
    "Example:\n",
    "{{\"en\": \"monetary policy committee\", \"vi\": \"·ªßy ban ch√≠nh s√°ch ti·ªÅn t·ªá\", \"th\": \"\", \"ms\": \"jawatankuasa dasar monetari\"}}\n",
    "\n",
    "INPUT: {input_batch}\n",
    "\"\"\"\n",
    "\n",
    "# --- FUNCTIONS ---\n",
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        try:\n",
    "            with open(PROGRESS_FILE, \"r\", encoding=\"utf8\") as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return {\"train_index\": 0, \"test_index\": 0}\n",
    "    return {\"train_index\": 0, \"test_index\": 0}\n",
    "\n",
    "def save_progress(train_i, test_i):\n",
    "    with open(PROGRESS_FILE, \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump({\"train_index\": train_i, \"test_index\": test_i}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def safe_json_parse(text):\n",
    "    if not text: return None\n",
    "    text = re.sub(r\"```json|```\", \"\", text).strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        # Try regex to extract first JSON array\n",
    "        match = re.search(r\"\\[\\s*\\{.*?\\}\\s*\\]\", text, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(0))\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "def process_split(dataset, append_file, start_idx, split_name, progress):\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=MODEL_NAME,\n",
    "        generation_config={\"response_mime_type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(dataset), BATCH_SIZE), desc=split_name):\n",
    "        batch = dataset[i : i + BATCH_SIZE]\n",
    "        keys = batch.keys()\n",
    "        list_of_values = zip(*[batch[k] for k in keys])\n",
    "        batch_rows = []\n",
    "\n",
    "        for values in list_of_values:\n",
    "            row_dict = dict(zip(keys, values))\n",
    "            filtered_row = {k: row_dict[k] for k in TARGET_LANGS if k in row_dict}\n",
    "            batch_rows.append(filtered_row)\n",
    "\n",
    "        inp = json.dumps(batch_rows, ensure_ascii=False, indent=2)\n",
    "        prompt = PROMPT_TEMPLATE.format(input_batch=inp)\n",
    "\n",
    "        retry_count = 0\n",
    "        parsed = None\n",
    "\n",
    "        while retry_count < 5:\n",
    "            try:\n",
    "                response = model.generate_content(prompt)\n",
    "                parsed = safe_json_parse(response.text)\n",
    "                if parsed is not None and isinstance(parsed, list):\n",
    "                    break\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid JSON format\")\n",
    "            except Exception as e:\n",
    "                wait = (2 ** retry_count) + random.random()\n",
    "                print(f\"\\nError at index {i}: {e}. Retrying in {wait:.1f}s...\")\n",
    "                time.sleep(wait)\n",
    "                retry_count += 1\n",
    "\n",
    "        # Write output\n",
    "        with open(append_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            if parsed:\n",
    "                for item in parsed:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            else:\n",
    "                f.write(json.dumps({\n",
    "                    \"error\": \"Failed to parse\",\n",
    "                    \"input_index\": i,\n",
    "                    \"raw_output\": repr(response.text) if 'response' in locals() else \"\"\n",
    "                }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Save progress safely\n",
    "        if split_name == \"train\":\n",
    "            progress[\"train_index\"] = i + BATCH_SIZE\n",
    "        else:\n",
    "            progress[\"test_index\"] = i + BATCH_SIZE\n",
    "        save_progress(progress[\"train_index\"], progress[\"test_index\"])\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data = load_dataset(\"json\", data_files={\n",
    "            \"train\": \"alt_multilingual_dataset/train.jsonl\",\n",
    "            \"test\": \"alt_multilingual_dataset/test.jsonl\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Kh√¥ng t√¨m th·∫•y file dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    progress = load_progress()\n",
    "\n",
    "    if progress[\"train_index\"] == 0 and progress[\"test_index\"] == 0 and os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "\n",
    "    print(f\"=== Using Model: {MODEL_NAME} ===\")\n",
    "\n",
    "    if \"train\" in data:\n",
    "        print(\"=== TRAIN ===\")\n",
    "        process_split(data[\"train\"], OUTPUT_FILE, progress[\"train_index\"], \"train\", progress)\n",
    "\n",
    "    if \"test\" in data:\n",
    "        print(\"=== TEST ===\")\n",
    "        process_split(data[\"test\"], OUTPUT_FILE, progress[\"test_index\"], \"test\", progress)\n",
    "\n",
    "    print(f\"üéâ DONE! All results written to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098047c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Using Model: gemini-2.5-flash-lite ===\n",
      "=== TRAIN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   4%|‚ñç         | 37/859 [08:01<3:04:24, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at index 10887: Invalid JSON format. Retrying in 1.1s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  13%|‚ñà‚ñé        | 114/859 [28:26<2:49:41, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at index 11657: Invalid JSON format. Retrying in 1.0s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  14%|‚ñà‚ñé        | 116/859 [29:16<3:46:34, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at index 11677: Invalid JSON format. Retrying in 1.3s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  37%|‚ñà‚ñà‚ñà‚ñã      | 314/859 [1:15:34<1:36:40, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at index 13657: Invalid JSON format. Retrying in 1.8s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 398/859 [1:34:43<1:48:31, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at index 14497: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.. Retrying in 1.1s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 425/859 [1:44:03<1:33:50, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at index 14767: Invalid JSON format. Retrying in 1.7s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 521/859 [2:05:03<1:23:32, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at index 15727: Invalid JSON format. Retrying in 1.0s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 859/859 [3:26:32<00:00, 14.43s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 73/101 [15:58<06:28, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error at index 730: Invalid JSON format. Retrying in 1.5s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [22:10<00:00, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ DONE! All results written to phrases.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "API_KEY = \"AIzaSyASrqHq1u4IPLuc-ZBk1ueb0jZGJcTZFU4\"\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Please set GEMINI_API_KEY environment variable.\")\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "BATCH_SIZE = 3  # Safe for Free Tier\n",
    "MAX_REQUESTS_PER_MIN = 15  # Free Tier limit\n",
    "MIN_WAIT_BETWEEN_REQUESTS = 60 / MAX_REQUESTS_PER_MIN\n",
    "\n",
    "OUTPUT_FILE = \"phrases.jsonl\"\n",
    "PROGRESS_FILE = \"progress.json\"\n",
    "TARGET_LANGS = ['en','vi','id','ms','fil','khm','lo','th','my']\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a Multilingual High-Value Term Extraction Expert.\n",
    "\n",
    "Input: A JSON object of parallel sentences by language code.\n",
    "\n",
    "Goal:\n",
    "Extract ONLY English high-value terms that are difficult for NMT models to translate correctly,\n",
    "then align the exact corresponding expression from each target language.\n",
    "\n",
    "1) English Term Extraction:\n",
    "- Include: domain-specific noun phrases (Engineering, Legal, Medical, Political, Business, Economics, Technology), institutional/program names, technical compounds, idioms often mistranslated by NMT.\n",
    "- Exclude: easy/common phrases, place names (unless part of a technical unit), conversational language, dates/times/quantities.\n",
    "\n",
    "2) Structure & Quality:\n",
    "- Prefer bigrams/trigrams; single words only if highly technical.\n",
    "- No phrase begins/ends with stopword.\n",
    "- No pronouns; must start with noun or adjective.\n",
    "\n",
    "3) Multilingual Alignment:\n",
    "- Return exact matching string from target sentence.\n",
    "- If omitted/generalized, return \"\".\n",
    "- For scripts without spaces (TH/LA/KM/MM), extract full semantic unit.\n",
    "\n",
    "4) Output:\n",
    "Return ONLY a JSON array of objects with keys for each language present.\n",
    "\n",
    "Example:\n",
    "{{\"en\": \"monetary policy committee\", \"vi\": \"·ªßy ban ch√≠nh s√°ch ti·ªÅn t·ªá\", \"th\": \"\", \"ms\": \"jawatankuasa dasar monetari\"}}\n",
    "\n",
    "INPUT: {input_batch}\n",
    "\"\"\"\n",
    "\n",
    "# --- FUNCTIONS ---\n",
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        try:\n",
    "            with open(PROGRESS_FILE, \"r\", encoding=\"utf8\") as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            pass\n",
    "    return {\"train_index\": 0, \"test_index\": 0}\n",
    "\n",
    "def save_progress(train_i, test_i):\n",
    "    with open(PROGRESS_FILE, \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump({\"train_index\": train_i, \"test_index\": test_i}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def safe_json_parse(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    text = re.sub(r\"```json|```\", \"\", text).strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        match = re.search(r\"\\[\\s*\\{.*?\\}\\s*\\]\", text, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(0))\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "def wait_for_quota(last_request_time):\n",
    "    elapsed = time.time() - last_request_time\n",
    "    if elapsed < MIN_WAIT_BETWEEN_REQUESTS:\n",
    "        time.sleep(MIN_WAIT_BETWEEN_REQUESTS - elapsed)\n",
    "\n",
    "def process_split(dataset, append_file, start_idx, split_name, progress):\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=MODEL_NAME,\n",
    "        generation_config={\"response_mime_type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    last_request_time = 0\n",
    "\n",
    "    for i in tqdm(range(start_idx, len(dataset), BATCH_SIZE), desc=split_name):\n",
    "        batch = dataset[i : i + BATCH_SIZE]\n",
    "        keys = batch.keys()\n",
    "        list_of_values = zip(*[batch[k] for k in keys])\n",
    "        batch_rows = []\n",
    "\n",
    "        for values in list_of_values:\n",
    "            row_dict = dict(zip(keys, values))\n",
    "            filtered_row = {k: row_dict[k] for k in TARGET_LANGS if k in row_dict}\n",
    "            batch_rows.append(filtered_row)\n",
    "\n",
    "        inp = json.dumps(batch_rows, ensure_ascii=False)\n",
    "        prompt = PROMPT_TEMPLATE.format(input_batch=inp)\n",
    "\n",
    "        retry_count = 0\n",
    "        parsed = None\n",
    "\n",
    "        while retry_count < 7:\n",
    "            wait_for_quota(last_request_time)\n",
    "            try:\n",
    "                response = model.generate_content(prompt)\n",
    "                last_request_time = time.time()\n",
    "\n",
    "                parsed = safe_json_parse(response.text)\n",
    "                if parsed is not None and isinstance(parsed, list):\n",
    "                    break\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid JSON format\")\n",
    "            except Exception as e:\n",
    "                wait = (2 ** retry_count) + random.random()\n",
    "                print(f\"\\nError at index {i}: {e}. Retrying in {wait:.1f}s...\")\n",
    "                time.sleep(wait)\n",
    "                retry_count += 1\n",
    "\n",
    "        # Write output\n",
    "        with open(append_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            if parsed:\n",
    "                for item in parsed:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "            else:\n",
    "                f.write(json.dumps({\n",
    "                    \"error\": \"Failed to parse\",\n",
    "                    \"input_index\": i,\n",
    "                    \"raw_output\": repr(response.text) if 'response' in locals() else \"\"\n",
    "                }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Save progress safely\n",
    "        if split_name == \"train\":\n",
    "            progress[\"train_index\"] = i + BATCH_SIZE\n",
    "        else:\n",
    "            progress[\"test_index\"] = i + BATCH_SIZE\n",
    "        save_progress(progress[\"train_index\"], progress[\"test_index\"])\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data = load_dataset(\"json\", data_files={\n",
    "            \"train\": \"alt_multilingual_dataset/train.jsonl\",\n",
    "            \"test\": \"alt_multilingual_dataset/test.jsonl\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Kh√¥ng t√¨m th·∫•y file dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    progress = load_progress()\n",
    "\n",
    "    if progress[\"train_index\"] == 0 and progress[\"test_index\"] == 0 and os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "\n",
    "    print(f\"=== Using Model: {MODEL_NAME} ===\")\n",
    "\n",
    "    if \"train\" in data:\n",
    "        print(\"=== TRAIN ===\")\n",
    "        process_split(data[\"train\"], OUTPUT_FILE, progress[\"train_index\"], \"train\", progress)\n",
    "\n",
    "    if \"test\" in data:\n",
    "        print(\"=== TEST ===\")\n",
    "        process_split(data[\"test\"], OUTPUT_FILE, progress[\"test_index\"], \"test\", progress)\n",
    "\n",
    "    print(f\"üéâ DONE! All results written to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3da314",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b5de57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19586 extracted phrases.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"phrases.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "print(f\"Loaded {len(data)} extracted phrases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a2695df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned: 461 items\n",
      "Missing: 141 items\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# -------- CONFIG ---------\n",
    "\n",
    "stopwords = {\"the\", \"a\", \"an\", \"to\", \"of\", \"in\"}\n",
    "pronouns = {\"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\"me\",\"him\",\"her\",\"us\",\"them\"}\n",
    "be_verbs = {\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"being\",\"been\"}\n",
    "\n",
    "fields = [\"en\", \"vi\", \"id\", \"ms\", \"fil\", \"khm\", \"lo\", \"th\", \"my\"]\n",
    "\n",
    "# -------- CLEANING RULES ---------\n",
    "\n",
    "def safe_text(val):\n",
    "    \"\"\"Chuy·ªÉn None -> '' ƒë·ªÉ tr√°nh l·ªói .strip()\"\"\"\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    return str(val)\n",
    "\n",
    "def contains_number_or_special(text):\n",
    "    return bool(re.search(r\"[0-9!@#$%^&*()_+=\\[\\]{};:\\\"\\\\|,.<>/?~]\", text))\n",
    "\n",
    "def starts_with_stopword(text):\n",
    "    tokens = text.lower().split()\n",
    "    if not tokens:\n",
    "        return False\n",
    "    return tokens[0] in stopwords\n",
    "\n",
    "def contains_pronoun(text):\n",
    "    tokens = set(text.lower().split())\n",
    "    return len(tokens & pronouns) > 0\n",
    "\n",
    "def contains_be_verb(text):\n",
    "    tokens = set(text.lower().split())\n",
    "    return len(tokens & be_verbs) > 0\n",
    "\n",
    "def is_bigram_or_trigram(text):\n",
    "    n = len(text.split())\n",
    "    return n != 1 \n",
    "\n",
    "def too_many_duplicates(entry, threshold=4):\n",
    "    \"\"\"X√≥a n·∫øu c√≥ >= 4 b·∫£n d·ªãch gi·ªëng nhau\"\"\"\n",
    "    texts = [safe_text(entry[f]).strip().lower() for f in fields if f in entry]\n",
    "    freq = {}\n",
    "    for t in texts:\n",
    "        freq[t] = freq.get(t, 0) + 1\n",
    "    return max(freq.values()) >= threshold\n",
    "\n",
    "# -------- PROCESSING ---------\n",
    "\n",
    "def clean_dataset(data):\n",
    "    cleaned = []\n",
    "    missing = []\n",
    "\n",
    "    for item in data:\n",
    "        # --- Check missing fields ---\n",
    "        has_all_fields = True\n",
    "        for f in fields:\n",
    "            if f not in item or safe_text(item[f]).strip() == \"\":\n",
    "                has_all_fields = False\n",
    "                break\n",
    "\n",
    "        if not has_all_fields:\n",
    "            missing.append(item)\n",
    "            continue\n",
    "\n",
    "        # --- Apply cleaning rules ---\n",
    "        delete = False\n",
    "        for f in fields:\n",
    "            text = safe_text(item[f])\n",
    "\n",
    "            if starts_with_stopword(text):\n",
    "                delete = True; break\n",
    "            if contains_number_or_special(text):\n",
    "                delete = True; break\n",
    "            if contains_pronoun(text):\n",
    "                delete = True; break\n",
    "            #if contains_be_verb(text):\n",
    "            #    delete = True; break\n",
    "            if not is_bigram_or_trigram(text):\n",
    "                delete = True; break\n",
    "\n",
    "        if too_many_duplicates(item, threshold=4):\n",
    "            delete = True\n",
    "\n",
    "        if not delete:\n",
    "            cleaned.append(item)\n",
    "\n",
    "    return cleaned, missing\n",
    "\n",
    "# -------- WRITE OUTPUT ---------\n",
    "\n",
    "def write_jsonl(path, items):\n",
    "    with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "        for obj in items:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "cleaned, missing = clean_dataset(data)\n",
    "\n",
    "write_jsonl(\"phrases_cleaned.jsonl\", cleaned)\n",
    "write_jsonl(\"missing.jsonl\", missing)\n",
    "\n",
    "print(\"Cleaned:\", len(cleaned), \"items\")\n",
    "print(\"Missing:\", len(missing), \"items\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e4fbc",
   "metadata": {},
   "source": [
    "# TEST RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630c50c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu t·∫°o DB...\n",
      " ...ƒë√£ insert 10000 d√≤ng.\n",
      "‚úÖ Database created at dictionary_ALT.db. Total unique entries: 10221\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "DB_PATH = \"dictionary_ALT.db\"\n",
    "\n",
    "# C√°c key trong JSON\n",
    "LANG_COLUMNS = ['en_np','vi','id','ms','fil','khm','lo','th','my']\n",
    "\n",
    "# Mapping t·ª´ JSON key sang t√™n c·ªôt DB\n",
    "DB_COLUMN_MAP = {\n",
    "    \"en_np\": \"English\",\n",
    "    \"id\": \"Indonesian\",\n",
    "    \"ms\": \"Malay (Malaysia)\",\n",
    "    \"fil\": \"Filipino / Tagalog\",\n",
    "    \"khm\": \"Khmer (Cambodian)\",\n",
    "    \"lo\": \"Lao\",\n",
    "    \"th\": \"Thai\",\n",
    "    \"my\": \"Burmese (Myanmar)\",\n",
    "    \"vi\": \"Vietnamese\"\n",
    "}\n",
    "\n",
    "def create_db_from_json(json_file, db_path=DB_PATH):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # X√≥a b·∫£ng c≈© n·∫øu t·ªìn t·∫°i\n",
    "    cur.execute(\"DROP TABLE IF EXISTS dict\")\n",
    "\n",
    "    # T·∫°o c·ªôt DB d·ª±a tr√™n mapping\n",
    "    columns_def = \", \".join([f\"[{DB_COLUMN_MAP[lang]}] TEXT\" for lang in LANG_COLUMNS])\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE dict (\n",
    "            entry_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            {columns_def}\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # T·∫°o index cho t·ª´ng c·ªôt\n",
    "    for lang in LANG_COLUMNS:\n",
    "        col_name = DB_COLUMN_MAP[lang]\n",
    "        safe_index_name = f\"idx_{col_name.replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '')}\"\n",
    "        # quote c·ªôt b·∫±ng d·∫•u nh√°y k√©p v√¨ t√™n c√≥ kho·∫£ng tr·∫Øng/ k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "        cur.execute(f'CREATE INDEX IF NOT EXISTS {safe_index_name} ON dict(\"{col_name}\")')\n",
    "\n",
    "\n",
    "    BATCH_SIZE = 10000\n",
    "    batch_data = []\n",
    "    unique_entries = set()\n",
    "\n",
    "    cols_formatted = \", \".join([f\"[{DB_COLUMN_MAP[l]}]\" for l in LANG_COLUMNS])\n",
    "    insert_query = f\"INSERT INTO dict ({cols_formatted}) VALUES ({', '.join(['?']*len(LANG_COLUMNS))})\"\n",
    "\n",
    "    print(\"üöÄ B·∫Øt ƒë·∫ßu t·∫°o DB...\")\n",
    "\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            row = [item.get(lang, \"\").strip() for lang in LANG_COLUMNS]\n",
    "            key_check = tuple(x.lower() for x in row)\n",
    "            if key_check not in unique_entries:\n",
    "                unique_entries.add(key_check)\n",
    "                batch_data.append(tuple(row))\n",
    "\n",
    "            if len(batch_data) >= BATCH_SIZE:\n",
    "                cur.executemany(insert_query, batch_data)\n",
    "                batch_data = []\n",
    "                print(f\" ...ƒë√£ insert {len(unique_entries)} d√≤ng.\")\n",
    "\n",
    "    if batch_data:\n",
    "        cur.executemany(insert_query, batch_data)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"‚úÖ Database created at {db_path}. Total unique entries: {len(unique_entries)}\")\n",
    "\n",
    "\n",
    "create_db_from_json(\"phrases_cleaned.jsonl\", DB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547a463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C√¢u g·ªëc: The Banking sector policy is crucial. The cat is distinct from a cation.\n",
      "‚úÖ T√¨m th·∫•y: 'policy' (G·ªëc: policy) -> D·ªãch: 'polisiya'\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import ahocorasick\n",
    "DB_PATH = \"dictionary_ALT.db\"\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ Load terms (Gi·ªØ nguy√™n case g·ªëc)\n",
    "# =========================\n",
    "def load_terms_for_automaton(source_lang, target_lang, db_path=DB_PATH):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    # L·∫•y d·ªØ li·ªáu g·ªëc\n",
    "    rows = conn.execute(f'''\n",
    "        SELECT \"{source_lang}\", \"{target_lang}\"\n",
    "        FROM dict\n",
    "        WHERE \"{source_lang}\" != '' AND \"{target_lang}\" != ''\n",
    "    ''').fetchall()\n",
    "    conn.close()\n",
    "    return rows\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ Build automaton (Key l√† ch·ªØ th∆∞·ªùng, Value gi·ªØ nguy√™n g·ªëc)\n",
    "# =========================\n",
    "def build_automaton(term_pairs):\n",
    "    A = ahocorasick.Automaton()\n",
    "    for src, tgt in term_pairs:\n",
    "        if src:\n",
    "            # Key add v√†o Automaton PH·∫¢I l√† ch·ªØ th∆∞·ªùng ƒë·ªÉ match case-insensitive\n",
    "            # Value l∆∞u l·∫°i (src_g·ªëc, tgt_g·ªëc) ƒë·ªÉ tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë·∫πp\n",
    "            A.add_word(src.lower(), (src, tgt)) \n",
    "    A.make_automaton()\n",
    "    return A\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4Ô∏è‚É£ Query (S·ª¨A L·ªñI LOGIC QUAN TR·ªåNG: Word Boundary)\n",
    "# =========================\n",
    "def query_terms_in_sentence(sentence, automaton):\n",
    "    sentence_lower = sentence.lower()\n",
    "    results = []\n",
    "    \n",
    "    # H√†m ki·ªÉm tra k√Ω t·ª± c√≥ ph·∫£i l√† ch·ªØ/s·ªë kh√¥ng\n",
    "    def is_word_char(char):\n",
    "        return char.isalnum() or char == '_'\n",
    "\n",
    "    # Aho-Corasick tr·∫£ v·ªÅ end_index (v·ªã tr√≠ k√Ω t·ª± cu·ªëi c√πng c·ªßa t·ª´ match)\n",
    "    for end_idx, (original_src, original_tgt) in automaton.iter(sentence_lower):\n",
    "        matched_len = len(original_src)\n",
    "        start_idx = end_idx - matched_len + 1\n",
    "        \n",
    "        # 1. Ki·ªÉm tra bi√™n tr√°i (k√Ω t·ª± tr∆∞·ªõc t·ª´ t√¨m th·∫•y)\n",
    "        if start_idx > 0 and is_word_char(sentence_lower[start_idx - 1]):\n",
    "            continue # B·ªè qua v√¨ d√≠nh v√†o t·ª´ tr∆∞·ªõc (vd: match \"ba\" trong \"s√¢n_banh\")\n",
    "\n",
    "        # 2. Ki·ªÉm tra bi√™n ph·∫£i (k√Ω t·ª± sau t·ª´ t√¨m th·∫•y)\n",
    "        if end_idx < len(sentence_lower) - 1 and is_word_char(sentence_lower[end_idx + 1]):\n",
    "            continue # B·ªè qua v√¨ d√≠nh v√†o t·ª´ sau (vd: match \"cat\" trong \"cation\")\n",
    "\n",
    "        # N·∫øu v∆∞·ª£t qua 2 check tr√™n -> ƒê√¢y l√† t·ª´ nguy√™n v·∫πn\n",
    "        results.append({\n",
    "            \"start\": start_idx,\n",
    "            \"end\": end_idx,\n",
    "            \"source_match\": original_src, # T·ª´ g·ªëc trong t·ª´ ƒëi·ªÉn\n",
    "            \"target\": original_tgt,       # Nghƒ©a g·ªëc\n",
    "            \"in_sentence\": sentence[start_idx:end_idx+1] # T·ª´ th·ª±c t·∫ø trong c√¢u (gi·ªØ case c·ªßa c√¢u)\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5Ô∏è‚É£ V√≠ d·ª• s·ª≠ d·ª•ng\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    source_lang = \"English\"\n",
    "    target_lang = \"Filipino / Tagalog\"\n",
    "    term_pairs = load_terms_for_automaton(source_lang, target_lang)\n",
    "    automaton = build_automaton(term_pairs)\n",
    "\n",
    "\n",
    "    sentence = \"The Banking sector policy is crucial. The cat is distinct from a cation.\"\n",
    "    \n",
    "    print(f\"\\nC√¢u g·ªëc: {sentence}\")\n",
    "    matched_terms = query_terms_in_sentence(sentence, automaton)\n",
    "    \n",
    "    for m in matched_terms:\n",
    "        print(f\"‚úÖ T√¨m th·∫•y: '{m['in_sentence']}' (G·ªëc: {m['source_match']}) -> D·ªãch: '{m['target']}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
