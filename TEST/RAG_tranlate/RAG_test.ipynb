{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b592e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"phuong123/icd_icf_en_vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454bb564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí N·∫°p 21555 d√≤ng t·ª´ ngu·ªìn icd_icf\n",
      "‚Üí N·∫°p 16808 d√≤ng t·ª´ ngu·ªìn dictionary\n",
      "‚úÖ Database created successfully at dictionary.db\n",
      "üìä T·ªïng s·ªë d√≤ng unique: 38363\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "DB_PATH = \"dictionary.db\"\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# === T·∫°o b·∫£ng ===\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dict (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    english TEXT,\n",
    "    vietnamese TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"CREATE INDEX IF NOT EXISTS idx_en ON dict(english)\")\n",
    "cur.execute(\"CREATE INDEX IF NOT EXISTS idx_vi ON dict(vietnamese)\")\n",
    "\n",
    "# === N·∫°p d·ªØ li·ªáu an to√†n ===\n",
    "insert_query = \"INSERT INTO dict (english, vietnamese) VALUES (?, ?)\"\n",
    "unique_entries = set()\n",
    "\n",
    "for source_name, dataset in data.items():\n",
    "    if source_name != \"adapt\":  # b·ªè qua n·∫øu kh√¥ng c·∫ßn\n",
    "        count_before = len(unique_entries)\n",
    "        entries_to_insert = []\n",
    "        \n",
    "        for item in dataset:\n",
    "            en = item.get(\"en\")\n",
    "            vi = item.get(\"vi\")\n",
    "            \n",
    "            # B·ªè qua n·∫øu thi·∫øu\n",
    "            if not en or not vi:\n",
    "                continue\n",
    "            \n",
    "            # Chu·∫©n h√≥a\n",
    "            en = en.strip().lower() if isinstance(en, str) else \"\"\n",
    "            vi = vi.strip().lower() if isinstance(vi, str) else \"\"\n",
    "            \n",
    "            key = (en, vi)\n",
    "            if key in unique_entries:\n",
    "                continue\n",
    "            unique_entries.add(key)\n",
    "            entries_to_insert.append((en, vi))\n",
    "        \n",
    "        # Insert nhi·ªÅu d√≤ng c√πng l√∫c\n",
    "        cur.executemany(insert_query, entries_to_insert)\n",
    "        count_after = len(unique_entries)\n",
    "        print(f\"‚Üí N·∫°p {count_after - count_before} d√≤ng t·ª´ ngu·ªìn {source_name}\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(f\"‚úÖ Database created successfully at {DB_PATH}\")\n",
    "print(f\"üìä T·ªïng s·ªë d√≤ng unique: {len(unique_entries)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96da8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import ahocorasick\n",
    "\n",
    "DB_PATH = \"dictionary.db\"\n",
    "\n",
    "# === H√†m kh·ªüi t·∫°o k·∫øt n·ªëi ===\n",
    "def get_connection():\n",
    "    return sqlite3.connect(DB_PATH)\n",
    "\n",
    "# === Tra 1 t·ª´ (Anh ho·∫∑c Vi·ªát) ===\n",
    "def translate(word):\n",
    "    conn = get_connection()\n",
    "    word = word.strip().lower()\n",
    "    rows = conn.execute(\"\"\"\n",
    "        SELECT english, vietnamese\n",
    "        FROM dict\n",
    "        WHERE english = ? OR vietnamese = ?\n",
    "        LIMIT 20\n",
    "    \"\"\", (word, word)).fetchall()\n",
    "    conn.close()\n",
    "    return [{\"en\": r[0], \"vi\": r[1]} for r in rows]\n",
    "\n",
    "# === T·∫£i to√†n b·ªô t·ª´ ƒëi·ªÉn v√†o b·ªô nh·ªõ (ƒë·ªÉ tra nhanh trong c√¢u) ===\n",
    "def load_all_terms():\n",
    "    conn = get_connection()\n",
    "    rows = conn.execute(\"SELECT english, vietnamese FROM dict\").fetchall()\n",
    "    conn.close()\n",
    "    return [(en, vi) for en, vi in rows]\n",
    "\n",
    "# === X√¢y b·ªô t√¨m ki·∫øm Aho‚ÄìCorasick ===\n",
    "def build_automaton(term_pairs):\n",
    "    A = ahocorasick.Automaton()\n",
    "    for en, vi in term_pairs:\n",
    "        if en:  # ch·ªâ th√™m n·∫øu kh√¥ng r·ªóng\n",
    "            A.add_word(en, (en, vi))\n",
    "    A.make_automaton()\n",
    "    return A\n",
    "\n",
    "# === T√¨m c√°c thu·∫≠t ng·ªØ trong c√¢u ===\n",
    "def find_terms_in_sentence(sentence, automaton):\n",
    "    sentence = sentence.lower()\n",
    "    results = []\n",
    "    seen = set()\n",
    "    for end_idx, (en, vi) in automaton.iter(sentence):\n",
    "        key = (en, vi)\n",
    "        if key not in seen and key[0] in sentence:\n",
    "            seen.add(key)\n",
    "            results.append({\"en\": en, \"vi\": vi})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53bc9c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'en': 'buff', 'vi': 'da tr√¢u, da b√≤'}]\n",
      "[{'en': 'sodium', 'vi': 'ch·∫•t c∆° b·∫£n c√≥ trong mu·ªëi'}, {'en': 'buff', 'vi': 'da tr√¢u, da b√≤'}, {'en': 'buffer', 'vi': 'ch·∫•t c√¢n b·∫±ng t√¨nh tr·∫°ng toan-ki·ªÅm'}, {'en': 'ion', 'vi': 'ch·∫•t nguy√™n t·ª≠ ion'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1Ô∏è‚É£ Tra 1 t·ª´\n",
    "print(translate(\"buff\"))\n",
    "# ‚Üí [{'english': 'heart', 'vietnamese': 'tim', 'source': 'dictionary'}]\n",
    "\n",
    "# 2Ô∏è‚É£ T√¨m thu·∫≠t ng·ªØ trong c√¢u\n",
    "terms = load_all_terms()\n",
    "A = build_automaton(terms)\n",
    "\n",
    "sentence = \"After precipating, diclofenac sodium was eluted by HPLC using a reverse-phase column (C8, 150 mm x 4.6 mm, 5 Œºm), mobile phase contains methanol / phosphat buffer (70: 30 v / v, pH 2.5), at a flow rate of 1.0 ml / min, and a wavelength detection at 275 nm.\"\n",
    "print(find_terms_in_sentence(sentence, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c67eb",
   "metadata": {},
   "source": [
    "# RAG_loop_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f80ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "ds = load_dataset(\"thviet79/MT_Medical\")\n",
    "\n",
    "def split_parallel(dataset):\n",
    "    n = len(dataset) // 2\n",
    "    en_texts = dataset[\"text\"][:n]\n",
    "    vi_texts = dataset[\"text\"][n:]\n",
    "    return Dataset.from_dict({\"en\": en_texts, \"vi\": vi_texts})\n",
    "\n",
    "test_split = split_parallel(ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cf0d976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:00<00:00, 30302.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "output = []\n",
    "terms = load_all_terms()\n",
    "A = build_automaton(terms)\n",
    "for item in tqdm(test_split):\n",
    "    output.append({\"en\": item[\"en\"], \n",
    "                   \"dictionary\" : find_terms_in_sentence(item[\"en\"], A), \n",
    "                   \"vi\": item[\"vi\"]}\n",
    "                   )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1583d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"RAG_test_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2a300de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(batch_en, dictory):\n",
    "    # Gh√©p t·∫•t c·∫£ dictionary trong batch th√†nh text\n",
    "    term_lines = []\n",
    "    seen = set()\n",
    "    for term_list in dictory:\n",
    "        if term_list:  \n",
    "            for term in term_list:\n",
    "                en = term.get(\"en\", \"\").strip()\n",
    "                vi = term.get(\"vi\", \"\").strip()\n",
    "                if en and vi and (en, vi) not in seen:\n",
    "                    term_lines.append(f\"- {en} ‚Üí {vi}\")\n",
    "                    seen.add((en, vi))\n",
    "    term_text = \"\\n\".join(term_lines[:100])\n",
    "    \n",
    "    # Gh√©p c√°c c√¢u c·∫ßn d·ªãch\n",
    "    sentences_text = \"\\n\".join([f\"{s}\" for s in batch_en])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        B·∫°n l√† chuy√™n gia d·ªãch thu·∫≠t y h·ªçc Anh‚ÄìVi·ªát, c√≥ kinh nghi·ªám bi√™n t·∫≠p b√†i b√°o khoa h·ªçc v√† b√°o c√°o nghi√™n c·ª©u trong c√°c lƒ©nh v·ª±c Y h·ªçc, D∆∞·ª£c h·ªçc, v√† S·ª©c kh·ªèe c·ªông ƒë·ªìng.\n",
    "\n",
    "        Nhi·ªám v·ª•:\n",
    "        - D·ªãch c√°c c√¢u ti·∫øng Anh sang ti·∫øng Vi·ªát h·ªçc thu·∫≠t, ƒë·∫£m b·∫£o **ƒë·∫ßy ƒë·ªß th√¥ng tin**, kh√¥ng b·ªè s√≥t hay r√∫t g·ªçn.\n",
    "        - M·ªói m·ªánh ƒë·ªÅ, c·ª•m danh t·ª´, c·ª•m ƒë·ªông t·ª´ trong ti·∫øng Anh ph·∫£i c√≥ ph·∫ßn t∆∞∆°ng ·ª©ng trong b·∫£n d·ªãch.\n",
    "        - C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh tr·∫≠t t·ª± c√¢u ƒë·ªÉ t·ª± nhi√™n h∆°n trong ti·∫øng Vi·ªát, **nh∆∞ng kh√¥ng l√†m thay ƒë·ªïi quan h·ªá ng·ªØ nghƒ©a**.\n",
    "        - ∆Øu ti√™n s·ª≠ d·ª•ng ƒë√∫ng c√°c thu·∫≠t ng·ªØ y khoa trong danh s√°ch sau:\n",
    "        {term_text}\n",
    "\n",
    "        Y√™u c·∫ßu:\n",
    "        - Gi·ªØ nguy√™n √Ω nghƒ©a, c·∫•u tr√∫c ng·ªØ ph√°p t∆∞∆°ng ·ª©ng gi·ªØa hai ng√¥n ng·ªØ.\n",
    "        - Di·ªÖn ƒë·∫°t tr√¥i ch·∫£y, mang phong c√°ch h·ªçc thu·∫≠t ti·∫øng Vi·ªát.\n",
    "        - Thu·∫≠t ng·ªØ y h·ªçc ph·∫£i ch√≠nh x√°c v√† th·ªëng nh·∫•t.\n",
    "        - Ch·ªâ tr·∫£ v·ªÅ b·∫£n d·ªãch ti·∫øng Vi·ªát ho√†n ch·ªânh, kh√¥ng k√®m ch√∫ th√≠ch ho·∫∑c ph√¢n t√≠ch.\n",
    "\n",
    "        V√≠ d·ª•:\n",
    "        English: \"Mice in each group were assessed for weight weekly and the levels of Total Cholesterol (CT), HDL-Cholesterol (HDL-C), LDL-Cholesterol (LDL-C) and Triglyceride (TG) were recorded at initial time (after obesity was induced for 8 weeks) and 1 hour after taking the extracted mixtures on the last day.\"\n",
    "        Vietnamese: \"Tr·ªçng l∆∞·ª£ng chu·ªôt ·ªü m·ªói nh√≥m ƒë∆∞·ª£c ƒë√°nh gi√° h√†ng tu·∫ßn v√† c√°c ch·ªâ s·ªë Cholesterol to√†n ph·∫ßn (CT), HDL-Cholesterol (HDL-C), LDL-Cholesterol (LDL-C) v√† Triglycerid (TG) ƒë∆∞·ª£c ghi nh·∫≠n t·∫°i th·ªùi ƒëi·ªÉm ban ƒë·∫ßu (sau 8 tu·∫ßn g√¢y b√©o ph√¨) v√† 1 gi·ªù sau khi u·ªëng h·ªón h·ª£p chi·∫øt xu·∫•t v√†o ng√†y cu·ªëi c√πng.\"\n",
    "\n",
    "        B√¢y gi·ªù, h√£y d·ªãch c√°c c√¢u sau:\n",
    "        {sentences_text}\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ec26825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [35:06<00:00,  5.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"RAG_test_output_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_test = json.load(f)\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyB5zDpFEqzEQmBGK3axkLSqUKbNiUxzUWQ\")\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "model = genai.GenerativeModel(MODEL_NAME)\n",
    "\n",
    "output = []\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(data_test), batch_size)):\n",
    "    translations = data_test[i : min(i + batch_size, len(data_test))]\n",
    "    \n",
    "    batch_en = [t[\"en\"] for t in translations]\n",
    "    batch_vi_label = [t[\"vi\"] for t in translations]\n",
    "    dictory = [t[\"dictionary\"] for t in translations]\n",
    "    \n",
    "    prompt = build_prompt(batch_en, dictory)\n",
    "    #print(f\"--- Prompt for batch {i} ---\")\n",
    "    #print(prompt)\n",
    "    \n",
    "    # G·ªçi model\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": 0,\n",
    "                \"max_output_tokens\": 2048\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response and response.text:\n",
    "            vi_texts = response.text.strip()\n",
    "            # T√°ch v√† l√†m s·∫°ch k·∫øt qu·∫£\n",
    "            translations = []\n",
    "            for line in vi_texts.split(\"\\n\"):\n",
    "                line = line.strip()\n",
    "                # Lo·∫°i b·ªè s·ªë th·ª© t·ª± n·∫øu c√≥\n",
    "                if line and line[0].isdigit() and '.' in line[:3]:\n",
    "                    line = line.split('.', 1)[1].strip()\n",
    "                if line:\n",
    "                    translations.append(line)\n",
    "            \n",
    "            # Ki·ªÉm tra v√† c√¢n b·∫±ng s·ªë l∆∞·ª£ng\n",
    "            if len(translations) < len(batch_en):\n",
    "                translations.extend([\"[MISSING]\"] * (len(batch_en) - len(translations)))\n",
    "            elif len(translations) > len(batch_en):\n",
    "                translations = translations[:len(batch_en)]\n",
    "                \n",
    "        else:\n",
    "            translations = [\"[EMPTY_RESPONSE]\"] * len(batch_en)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói khi d·ªãch batch {i}: {e}\")\n",
    "        translations = [\"[ERROR]\"] * len(batch_en)\n",
    "    \n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    for k in range(len(batch_en)):\n",
    "        output.append({\n",
    "            \"en\": batch_en[k],\n",
    "            \"vi_pred\": translations[k],\n",
    "            \"vi_label\": batch_vi_label[k]\n",
    "        })\n",
    "    \n",
    "    # Ngh·ªâ gi·ªØa c√°c batch\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f307edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"RAG_test_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
